{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "468658b1-2bf6-49cf-b50e-7b8cd89dd2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Initializing Spark session...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/31 14:39:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized\n",
      "\n",
      "Verifying existing Iceberg table:\n",
      "\n",
      "Total records:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   count|\n",
      "+--------+\n",
      "|77966324|\n",
      "+--------+\n",
      "\n",
      "\n",
      "Sample data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2023-10-12 00:11:15|  2023-10-12 00:41:14|            1.0|        10.22|       1.0|                 N|         132|          61|           1|       43.6|  1.0|    0.5|      9.57|         0.0|                  1.0|       57.42|                 0.0|       1.75|\n",
      "|       2| 2023-10-12 00:01:00|  2023-10-12 00:23:18|            2.0|        15.48|       1.0|                 N|         132|          16|           1|       58.3|  1.0|    0.5|       8.0|         0.0|                  1.0|       70.55|                 0.0|       1.75|\n",
      "|       2| 2023-10-12 00:24:48|  2023-10-13 00:02:36|            1.0|        17.86|       2.0|                 N|         132|          68|           1|       70.0|  0.0|    0.5|     16.19|        6.94|                  1.0|       97.13|                 2.5|        0.0|\n",
      "|       1| 2023-10-12 00:30:53|  2023-10-12 00:44:02|            1.0|          7.2|       1.0|                 N|         138|         262|           1|       28.2|10.25|    0.5|       9.4|        6.94|                  1.0|       56.29|                 2.5|       1.75|\n",
      "|       1| 2023-10-12 00:20:28|  2023-10-12 00:28:41|            0.0|          2.6|       1.0|                 N|         239|          75|           2|       12.8|  3.5|    0.5|       0.0|         0.0|                  1.0|        17.8|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "from benchmark_utils import BenchmarkResult\n",
    "\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "\n",
    "# Cell 2: Initialize Spark session with proper configurations\n",
    "def init_spark_session():\n",
    "    \"\"\"Initialize Spark session with Iceberg support\"\"\"\n",
    "    print(\"Initializing Spark session...\")\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"IcebergBenchmark\") \\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "        .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "        .config(\"spark.sql.catalog.demo.type\", \"rest\") \\\n",
    "        .config(\"spark.sql.defaultCatalog\", \"demo\") \\\n",
    "        .config(\"spark.sql.catalog.demo.uri\", \"http://rest:8181\") \\\n",
    "        .config(\"spark.sql.catalog.demo.warehouse\", \"s3://warehouse/\") \\\n",
    "        .config(\"spark.sql.catalog.demo.io-impl\", \"org.apache.iceberg.aws.s3.S3FileIO\") \\\n",
    "        .config(\"spark.sql.catalog.demo.s3.endpoint\", \"http://minio:9000\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"Spark session initialized\")\n",
    "    return spark\n",
    "\n",
    "# Initialize Spark\n",
    "spark = init_spark_session()\n",
    "\n",
    "# Cell 3: Verify existing table\n",
    "print(\"\\nVerifying existing Iceberg table:\")\n",
    "print(\"\\nTotal records:\")\n",
    "spark.sql(\"SELECT COUNT(*) as count FROM demo.nyc.taxis\").show()\n",
    "\n",
    "print(\"\\nSample data:\")\n",
    "spark.sql(\"SELECT * FROM demo.nyc.taxis LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25067fc-33bf-43b7-a7a5-415fb04d49af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensuring namespace exists...\n",
      "\n",
      "Available catalogs:\n",
      "+-------------+\n",
      "|      catalog|\n",
      "+-------------+\n",
      "|         demo|\n",
      "|spark_catalog|\n",
      "+-------------+\n",
      "\n",
      "\n",
      "Available namespaces in demo:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|      nyc|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Create namespace if it doesn't exist\n",
    "def ensure_namespace_exists():\n",
    "    \"\"\"Create the namespace if it doesn't exist\"\"\"\n",
    "    print(\"\\nEnsuring namespace exists...\")\n",
    "    \n",
    "    # Create namespace\n",
    "    spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.nyc\")\n",
    "    \n",
    "    # Verify catalogs and namespaces\n",
    "    print(\"\\nAvailable catalogs:\")\n",
    "    spark.sql(\"SHOW CATALOGS\").show()\n",
    "    \n",
    "    print(\"\\nAvailable namespaces in demo:\")\n",
    "    spark.sql(\"SHOW NAMESPACES IN demo\").show()\n",
    "\n",
    "ensure_namespace_exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad913a0f-af8c-4209-a3b7-e45b750745e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing simple aggregation query...\n",
      "\n",
      "Query:\n",
      "\n",
      "        SELECT\n",
      "            COUNT(*) AS total_records,\n",
      "            AVG(trip_distance) AS avg_trip_distance,\n",
      "            MAX(total_amount) AS max_total_amount,\n",
      "            MIN(fare_amount) AS min_fare_amount\n",
      "        FROM demo.nyc.taxis\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==================================================>   (213 + 12) / 227]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query completed in 7.73 seconds\n",
      "\n",
      "Results:\n",
      "   total_records  avg_trip_distance  max_total_amount  min_fare_amount\n",
      "0       77966324           5.040317         401095.62     -133391414.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cell 4: Run simple query\n",
    "def run_simple_query(spark):\n",
    "    \"\"\"Run and time a simple aggregation query\"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            COUNT(*) AS total_records,\n",
    "            AVG(trip_distance) AS avg_trip_distance,\n",
    "            MAX(total_amount) AS max_total_amount,\n",
    "            MIN(fare_amount) AS min_fare_amount\n",
    "        FROM demo.nyc.taxis\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executing simple aggregation query...\")\n",
    "    print(\"\\nQuery:\")\n",
    "    print(query)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = spark.sql(query)\n",
    "    df = result.toPandas()\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nQuery completed in {duration:.2f} seconds\")\n",
    "    print(\"\\nResults:\")\n",
    "    print(df)\n",
    "    \n",
    "    return {\"duration\": duration, \"result\": df}\n",
    "\n",
    "# Run simple query\n",
    "simple_results = run_simple_query(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8b8e162-b532-4794-913b-3bf929220866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing complex analysis query...\n",
      "\n",
      "Query:\n",
      "\n",
      "        SELECT \n",
      "            payment_type,\n",
      "            COUNT(*) as trip_count,\n",
      "            AVG(total_amount) as avg_total_amount,\n",
      "            MAX(tip_amount) as max_tip_amount,\n",
      "            SUM(CASE WHEN passenger_count > 1 THEN 1 ELSE 0 END) as shared_rides\n",
      "        FROM demo.nyc.taxis\n",
      "        WHERE trip_distance > 2 AND total_amount > 0\n",
      "        GROUP BY payment_type\n",
      "        ORDER BY trip_count DESC\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query completed in 9.76 seconds\n",
      "\n",
      "Results:\n",
      "   payment_type  trip_count  avg_total_amount  max_tip_amount  shared_rides\n",
      "0             1    27504582         37.871267         1400.16       6696053\n",
      "1             2     5922309         33.125444           32.00       1745599\n",
      "2             0     1603076         35.306151           92.35             0\n",
      "3             4      134663         42.777958           95.00         32343\n",
      "4             3       90810         33.755191           90.00         20430\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Run complex query\n",
    "def run_complex_query(spark):\n",
    "    \"\"\"Run and time a complex analysis query\"\"\"\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            payment_type,\n",
    "            COUNT(*) as trip_count,\n",
    "            AVG(total_amount) as avg_total_amount,\n",
    "            MAX(tip_amount) as max_tip_amount,\n",
    "            SUM(CASE WHEN passenger_count > 1 THEN 1 ELSE 0 END) as shared_rides\n",
    "        FROM demo.nyc.taxis\n",
    "        WHERE trip_distance > 2 AND total_amount > 0\n",
    "        GROUP BY payment_type\n",
    "        ORDER BY trip_count DESC\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executing complex analysis query...\")\n",
    "    print(\"\\nQuery:\")\n",
    "    print(query)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    result = spark.sql(query)\n",
    "    df = result.toPandas()\n",
    "    duration = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nQuery completed in {duration:.2f} seconds\")\n",
    "    print(\"\\nResults:\")\n",
    "    print(df)\n",
    "    \n",
    "    return {\"duration\": duration, \"result\": df}\n",
    "\n",
    "# Run complex query\n",
    "complex_results = run_complex_query(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84a89c0-ad3d-4e0c-8804-8f8d0ba2e4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing advanced fare trend analysis (Spark)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis completed in 16.22 seconds\n",
      "Total records analyzed: 125\n",
      "\n",
      "Results preview (most recent month):\n",
      "       month  payment_type  avg_base_fare  avg_total_fare  interpolated_tip  \\\n",
      "0 2024-01-01             1          72.96           85.72              8.67   \n",
      "1 2024-01-01             2           6.50           11.50              0.00   \n",
      "2 2023-12-01             0          22.90           28.99              1.77   \n",
      "3 2023-12-01             1          20.02           30.12              4.49   \n",
      "4 2023-12-01             2          20.10           25.41              0.00   \n",
      "\n",
      "   num_rides  fare_stddev  fare_change  fare_change_pct   trend_category  \n",
      "0          5        40.06        52.94            264.4  Strong Increase  \n",
      "1          1          NaN       -13.60            -67.7  Strong Decrease  \n",
      "2     176966        14.13         1.11              5.1  Strong Increase  \n",
      "3    2574324        17.90        -0.04             -0.2           Stable  \n",
      "4     536489        19.89         0.11              0.6           Stable  \n",
      "\n",
      "Summary Statistics:\n",
      "Average base fare: $19.49\n",
      "Average total fare: $25.46\n",
      "Average tip: $1.47\n",
      "\n",
      "Trend Distribution:\n",
      "trend_category\n",
      "Moderate Increase    39\n",
      "Strong Increase      27\n",
      "Moderate Decrease    24\n",
      "Stable               19\n",
      "Strong Decrease      16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def run_advanced_analysis_spark(spark):\n",
    "    \"\"\"Run advanced fare analysis with trend detection and interpolation for Spark\n",
    "    \n",
    "    Args:\n",
    "        spark: Spark session\n",
    "    Returns:\n",
    "        dict: Contains duration and results dataframe\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    WITH fare_stats AS (\n",
    "        SELECT \n",
    "            date_trunc('month', tpep_pickup_datetime) as month,\n",
    "            payment_type,\n",
    "            AVG(fare_amount) as avg_base_fare,\n",
    "            AVG(total_amount) as avg_total_fare,\n",
    "            AVG(tip_amount) as avg_tip,\n",
    "            COUNT(*) as num_rides,\n",
    "            STDDEV(fare_amount) as fare_stddev\n",
    "        FROM demo.nyc.taxis\n",
    "        WHERE fare_amount > 0 \n",
    "        AND total_amount > 0\n",
    "        AND YEAR(tpep_pickup_datetime) >= 2022\n",
    "        GROUP BY \n",
    "            date_trunc('month', tpep_pickup_datetime),\n",
    "            payment_type\n",
    "    ),\n",
    "    interpolated_points AS (\n",
    "        SELECT \n",
    "            month,\n",
    "            payment_type,\n",
    "            avg_base_fare,\n",
    "            avg_total_fare,\n",
    "            CASE \n",
    "                WHEN avg_tip IS NULL THEN \n",
    "                    LAG(avg_tip, 1) OVER (PARTITION BY payment_type ORDER BY month) +\n",
    "                    (LEAD(avg_tip, 1) OVER (PARTITION BY payment_type ORDER BY month) -\n",
    "                     LAG(avg_tip, 1) OVER (PARTITION BY payment_type ORDER BY month)) * 0.5\n",
    "                ELSE avg_tip\n",
    "            END as interpolated_tip,\n",
    "            num_rides,\n",
    "            fare_stddev,\n",
    "            avg_base_fare - LAG(avg_base_fare) OVER (PARTITION BY payment_type ORDER BY month) as fare_change,\n",
    "            100.0 * (avg_base_fare - LAG(avg_base_fare) OVER (PARTITION BY payment_type ORDER BY month)) / \n",
    "                NULLIF(LAG(avg_base_fare) OVER (PARTITION BY payment_type ORDER BY month), 0) as fare_change_pct\n",
    "        FROM fare_stats\n",
    "    )\n",
    "    SELECT \n",
    "        month,\n",
    "        payment_type,\n",
    "        ROUND(avg_base_fare, 2) as avg_base_fare,\n",
    "        ROUND(avg_total_fare, 2) as avg_total_fare,\n",
    "        ROUND(interpolated_tip, 2) as interpolated_tip,\n",
    "        num_rides,\n",
    "        ROUND(fare_stddev, 2) as fare_stddev,\n",
    "        ROUND(fare_change, 2) as fare_change,\n",
    "        ROUND(fare_change_pct, 1) as fare_change_pct,\n",
    "        CASE \n",
    "            WHEN fare_change_pct > 5 THEN 'Strong Increase'\n",
    "            WHEN fare_change_pct BETWEEN 1 AND 5 THEN 'Moderate Increase'\n",
    "            WHEN fare_change_pct BETWEEN -1 AND 1 THEN 'Stable'\n",
    "            WHEN fare_change_pct BETWEEN -5 AND -1 THEN 'Moderate Decrease'\n",
    "            ELSE 'Strong Decrease'\n",
    "        END as trend_category\n",
    "    FROM interpolated_points\n",
    "    ORDER BY month DESC, payment_type;\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Executing advanced fare trend analysis (Spark)...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = spark.sql(query)\n",
    "        df = result.toPandas()\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        print(f\"\\nAnalysis completed in {duration:.2f} seconds\")\n",
    "        print(f\"Total records analyzed: {len(df):,}\")\n",
    "        print(\"\\nResults preview (most recent month):\")\n",
    "        print(df.head())\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        print(\"\\nSummary Statistics:\")\n",
    "        print(f\"Average base fare: ${df['avg_base_fare'].mean():.2f}\")\n",
    "        print(f\"Average total fare: ${df['avg_total_fare'].mean():.2f}\")\n",
    "        print(f\"Average tip: ${df['interpolated_tip'].mean():.2f}\")\n",
    "        print(\"\\nTrend Distribution:\")\n",
    "        print(df['trend_category'].value_counts())\n",
    "        \n",
    "        return {\"duration\": duration, \"result\": df}\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing Spark analysis: {str(e)}\")\n",
    "        return None\n",
    "# Run advanced analysis\n",
    "advanced_results = run_advanced_analysis_spark(spark)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3c4ef1d-e18e-40fc-b4c9-fa7a06f301bc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark results saved to spark_benchmark_results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save benchmark results\n",
    "result = BenchmarkResult(\"Spark\", spark.version)\n",
    "\n",
    "# For simple query:\n",
    "total_records = simple_results[\"result\"][\"total_records\"].iloc[0]  # Get the COUNT(*) result\n",
    "result.add_query_result(\n",
    "    \"simple_aggregation\",\n",
    "    simple_results[\"duration\"],\n",
    "    total_records,\n",
    "    simple_results[\"result\"]\n",
    ")\n",
    "\n",
    "# For complex query:\n",
    "total_records = complex_results[\"result\"][\"trip_count\"].sum()  # Sum all group counts\n",
    "result.add_query_result(\n",
    "    \"complex_analysis\",\n",
    "    complex_results[\"duration\"],\n",
    "    total_records,\n",
    "    complex_results[\"result\"]\n",
    ")\n",
    "\n",
    "# Add to benchmark results\n",
    "result.add_query_result(\n",
    "    \"advanced_analysis\",\n",
    "    advanced_results[\"duration\"],\n",
    "    advanced_results[\"result\"][\"num_rides\"].sum(),\n",
    "    advanced_results[\"result\"]\n",
    ")\n",
    "\n",
    "# Save results (automatically captures resource usage)\n",
    "result.save('spark_benchmark_results.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1135666b-195d-43d2-bb75-f3e6d7270005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Spark session stopped\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Clean up\n",
    "spark.stop()\n",
    "print(\"\\nSpark session stopped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
