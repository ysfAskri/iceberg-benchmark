{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries and paths configured.\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pathlib import Path\n",
    "\n",
    "# Configure paths\n",
    "DATA_PATH = \"/home/iceberg/notebooks/data\"\n",
    "DATABASE_NAME = \"nyc\"\n",
    "TABLE_NAME = \"taxis\"\n",
    "\n",
    "print(\"Libraries and paths configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# # Define the URL of the JAR file\n",
    "# jar_url = \"https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.7.1/iceberg-spark-runtime-3.5_2.12-1.7.1.jar\"\n",
    "\n",
    "# # Define the local path to save the JAR file\n",
    "# jar_path = \"iceberg-spark-runtime-3.5_2.12-1.7.1.jar\"\n",
    "\n",
    "# # Download the JAR\n",
    "# print(f\"Downloading {jar_url}...\")\n",
    "# response = requests.get(jar_url, stream=True)\n",
    "# if response.status_code == 200:\n",
    "#     with open(jar_path, \"wb\") as file:\n",
    "#         for chunk in response.iter_content(chunk_size=1024):\n",
    "#             file.write(chunk)\n",
    "#     print(f\"Downloaded JAR to {jar_path}\")\n",
    "# else:\n",
    "#     print(f\"Failed to download JAR. HTTP Status Code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Spark session...\n",
      "Spark session initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/31 13:57:22 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize SparkSession\n",
    "def init_spark_session():\n",
    "    \"\"\"Initialize Spark session\"\"\"\n",
    "    print(\"Initializing Spark session...\")\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Jupyter\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Spark session initialized.\")\n",
    "    return spark\n",
    "\n",
    "spark = init_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.eventLog.enabled = true\n",
      "spark.driver.port = 34265\n",
      "spark.history.fs.logDirectory = /home/iceberg/spark-events\n",
      "spark.sql.warehouse.dir = file:/home/iceberg/notebooks/notebooks/spark-warehouse\n",
      "spark.sql.catalog.demo.s3.endpoint = http://minio:9000\n",
      "spark.eventLog.dir = /home/iceberg/spark-events\n",
      "spark.app.id = local-1735653439751\n",
      "spark.serializer.objectStreamReset = 100\n",
      "spark.master = local[*]\n",
      "spark.submit.deployMode = client\n",
      "spark.app.startTime = 1735653438288\n",
      "spark.driver.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.sql.catalogImplementation = in-memory\n",
      "spark.driver.host = 6fcc4cfe034b\n",
      "spark.sql.catalog.demo.warehouse = s3://warehouse/wh/\n",
      "spark.sql.catalog.demo.io-impl = org.apache.iceberg.aws.s3.S3FileIO\n",
      "spark.app.submitTime = 1735653438024\n",
      "spark.executor.id = driver\n",
      "spark.app.name = PySparkShell\n",
      "spark.sql.extensions = org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\n",
      "spark.sql.catalog.demo.uri = http://rest:8181\n",
      "spark.sql.catalog.demo.type = rest\n",
      "spark.rdd.compress = True\n",
      "spark.executor.extraJavaOptions = -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "spark.sql.catalog.demo = org.apache.iceberg.spark.SparkCatalog\n",
      "spark.sql.defaultCatalog = demo\n",
      "spark.submit.pyFiles = \n",
      "spark.ui.showConsoleProgress = true\n"
     ]
    }
   ],
   "source": [
    "# Display all Spark configurations\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"{item[0]} = {item[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 24 parquet files:\n",
      "- yellow_tripdata_2022-01.parquet\n",
      "- yellow_tripdata_2022-02.parquet\n",
      "- yellow_tripdata_2022-03.parquet\n",
      "- yellow_tripdata_2022-04.parquet\n",
      "- yellow_tripdata_2022-05.parquet\n",
      "- yellow_tripdata_2022-06.parquet\n",
      "- yellow_tripdata_2022-07.parquet\n",
      "- yellow_tripdata_2022-08.parquet\n",
      "- yellow_tripdata_2022-09.parquet\n",
      "- yellow_tripdata_2022-10.parquet\n",
      "- yellow_tripdata_2022-11.parquet\n",
      "- yellow_tripdata_2022-12.parquet\n",
      "- yellow_tripdata_2023-01.parquet\n",
      "- yellow_tripdata_2023-02.parquet\n",
      "- yellow_tripdata_2023-03.parquet\n",
      "- yellow_tripdata_2023-04.parquet\n",
      "- yellow_tripdata_2023-05.parquet\n",
      "- yellow_tripdata_2023-06.parquet\n",
      "- yellow_tripdata_2023-07.parquet\n",
      "- yellow_tripdata_2023-08.parquet\n",
      "- yellow_tripdata_2023-09.parquet\n",
      "- yellow_tripdata_2023-10.parquet\n",
      "- yellow_tripdata_2023-11.parquet\n",
      "- yellow_tripdata_2023-12.parquet\n",
      "Parquet files discovered: [PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-01.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-02.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-03.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-04.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-05.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-06.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-07.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-08.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-09.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-10.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-11.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2022-12.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-01.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-02.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-03.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-04.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-05.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-06.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-07.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-08.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-09.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-10.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-11.parquet'), PosixPath('/home/iceberg/notebooks/data/yellow_tripdata_2023-12.parquet')]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Discover all parquet files in the data directory\n",
    "def discover_parquet_files(data_path):\n",
    "    \"\"\"Find all parquet files in the data directory.\"\"\"\n",
    "    parquet_files = list(Path(data_path).glob(\"yellow_tripdata_*.parquet\"))\n",
    "    print(f\"Found {len(parquet_files)} parquet files:\")\n",
    "    for file in sorted(parquet_files):\n",
    "        print(f\"- {file.name}\")\n",
    "    return sorted(parquet_files)\n",
    "\n",
    "parquet_files = discover_parquet_files(DATA_PATH)\n",
    "print(f\"Parquet files discovered: {parquet_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Iceberg table...\n",
      "Iceberg table 'nyc.taxis' created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create an Iceberg table\n",
    "def create_iceberg_table(spark):\n",
    "    \"\"\"Create Iceberg table.\"\"\"\n",
    "    print(\"\\nCreating Iceberg table...\")\n",
    "\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    DROP TABLE IF EXISTS {DATABASE_NAME}.{TABLE_NAME}\n",
    "    \"\"\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_NAME}.{TABLE_NAME} (\n",
    "        VendorID              BIGINT,\n",
    "        tpep_pickup_datetime  TIMESTAMP,\n",
    "        tpep_dropoff_datetime TIMESTAMP,\n",
    "        passenger_count       DOUBLE,\n",
    "        trip_distance         DOUBLE,\n",
    "        RatecodeID            DOUBLE,\n",
    "        store_and_fwd_flag    STRING,\n",
    "        PULocationID          BIGINT,\n",
    "        DOLocationID          BIGINT,\n",
    "        payment_type          BIGINT,\n",
    "        fare_amount           DOUBLE,\n",
    "        extra                 DOUBLE,\n",
    "        mta_tax               DOUBLE,\n",
    "        tip_amount            DOUBLE,\n",
    "        tolls_amount          DOUBLE,\n",
    "        improvement_surcharge DOUBLE,\n",
    "        total_amount          DOUBLE,\n",
    "        congestion_surcharge  DOUBLE,\n",
    "        airport_fee           DOUBLE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (days(tpep_pickup_datetime))\n",
    "    TBLPROPERTIES (\n",
    "        'write.metadata.version-hint.enabled'='true',\n",
    "        'write.metadata.metrics.default'='truncate(16)',\n",
    "        'write.metadata.metrics.column.VendorID'='full',\n",
    "        'write.metadata.metrics.column.tpep_pickup_datetime'='full',\n",
    "        'format-version'='2',\n",
    "        'write.delete.mode'='merge-on-read',\n",
    "        'write.distribution-mode'='hash',\n",
    "        'write.parquet.compression-codec'='gzip'\n",
    "    )\n",
    "    \"\"\")\n",
    "    print(f\"Iceberg table '{DATABASE_NAME}.{TABLE_NAME}' created.\")\n",
    "\n",
    "create_iceberg_table(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iceberg table 'nyc.taxis' created with version hint configuration.\n",
      "\n",
      "Verifying table properties:\n",
      "Table Properties: Row(col_name='Table Properties', data_type='[current-snapshot-id=none,format=iceberg/parquet,format-version=2,write.delete.mode=merge-on-read,write.distribution-mode=hash,write.metadata.metrics.column.VendorID=full,write.metadata.metrics.column.tpep_pickup_datetime=full,write.metadata.metrics.default=truncate(16),write.metadata.version-hint.enabled=true,write.parquet.compression-codec=gzip]', comment='')\n"
     ]
    }
   ],
   "source": [
    " print(f\"Iceberg table '{DATABASE_NAME}.{TABLE_NAME}' created with version hint configuration.\")\n",
    "\n",
    "# Verify the table properties\n",
    "properties = spark.sql(f\"\"\"\n",
    "DESCRIBE TABLE EXTENDED {DATABASE_NAME}.{TABLE_NAME}\n",
    "\"\"\").collect()\n",
    "\n",
    "print(\"\\nVerifying table properties:\")\n",
    "for row in properties:\n",
    "    if 'Table Properties' in str(row):\n",
    "        print(f\"Table Properties: {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading data into Iceberg table...\n",
      "Loading yellow_tripdata_2022-01.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-01.parquet loaded.\n",
      "Loading yellow_tripdata_2022-02.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-02.parquet loaded.\n",
      "Loading yellow_tripdata_2022-03.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-03.parquet loaded.\n",
      "Loading yellow_tripdata_2022-04.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-04.parquet loaded.\n",
      "Loading yellow_tripdata_2022-05.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-05.parquet loaded.\n",
      "Loading yellow_tripdata_2022-06.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-06.parquet loaded.\n",
      "Loading yellow_tripdata_2022-07.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-07.parquet loaded.\n",
      "Loading yellow_tripdata_2022-08.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-08.parquet loaded.\n",
      "Loading yellow_tripdata_2022-09.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-09.parquet loaded.\n",
      "Loading yellow_tripdata_2022-10.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-10.parquet loaded.\n",
      "Loading yellow_tripdata_2022-11.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-11.parquet loaded.\n",
      "Loading yellow_tripdata_2022-12.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2022-12.parquet loaded.\n",
      "Loading yellow_tripdata_2023-01.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-01.parquet loaded.\n",
      "Loading yellow_tripdata_2023-02.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-02.parquet loaded.\n",
      "Loading yellow_tripdata_2023-03.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-03.parquet loaded.\n",
      "Loading yellow_tripdata_2023-04.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-04.parquet loaded.\n",
      "Loading yellow_tripdata_2023-05.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-05.parquet loaded.\n",
      "Loading yellow_tripdata_2023-06.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-06.parquet loaded.\n",
      "Loading yellow_tripdata_2023-07.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-07.parquet loaded.\n",
      "Loading yellow_tripdata_2023-08.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-08.parquet loaded.\n",
      "Loading yellow_tripdata_2023-09.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-09.parquet loaded.\n",
      "Loading yellow_tripdata_2023-10.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-10.parquet loaded.\n",
      "Loading yellow_tripdata_2023-11.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-11.parquet loaded.\n",
      "Loading yellow_tripdata_2023-12.parquet...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File yellow_tripdata_2023-12.parquet loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load data into Iceberg table\n",
    "def load_data_to_iceberg(spark, parquet_files):\n",
    "    \"\"\"Load data from Parquet files into Iceberg table.\"\"\"\n",
    "    print(\"\\nLoading data into Iceberg table...\")\n",
    "\n",
    "    for file in parquet_files:\n",
    "        print(f\"Loading {file.name}...\")\n",
    "        df = spark.read.parquet(str(file))\n",
    "        df.write.mode(\"append\").saveAsTable(f\"{DATABASE_NAME}.{TABLE_NAME}\")\n",
    "        print(f\"File {file.name} loaded.\")\n",
    "\n",
    "load_data_to_iceberg(spark, parquet_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verifying data in Iceberg table...\n",
      "Total rows in table: 77966324\n",
      "\n",
      "Sample data:\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2023-10-12 00:11:15|  2023-10-12 00:41:14|            1.0|        10.22|       1.0|                 N|         132|          61|           1|       43.6|  1.0|    0.5|      9.57|         0.0|                  1.0|       57.42|                 0.0|       1.75|\n",
      "|       2| 2023-10-12 00:01:00|  2023-10-12 00:23:18|            2.0|        15.48|       1.0|                 N|         132|          16|           1|       58.3|  1.0|    0.5|       8.0|         0.0|                  1.0|       70.55|                 0.0|       1.75|\n",
      "|       2| 2023-10-12 00:24:48|  2023-10-13 00:02:36|            1.0|        17.86|       2.0|                 N|         132|          68|           1|       70.0|  0.0|    0.5|     16.19|        6.94|                  1.0|       97.13|                 2.5|        0.0|\n",
      "|       1| 2023-10-12 00:30:53|  2023-10-12 00:44:02|            1.0|          7.2|       1.0|                 N|         138|         262|           1|       28.2|10.25|    0.5|       9.4|        6.94|                  1.0|       56.29|                 2.5|       1.75|\n",
      "|       1| 2023-10-12 00:20:28|  2023-10-12 00:28:41|            0.0|          2.6|       1.0|                 N|         239|          75|           2|       12.8|  3.5|    0.5|       0.0|         0.0|                  1.0|        17.8|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verify the data in the Iceberg table\n",
    "def verify_table_data(spark):\n",
    "    \"\"\"Verify data inside the Iceberg table.\"\"\"\n",
    "    print(\"\\nVerifying data in Iceberg table...\")\n",
    "\n",
    "    # Total rows count\n",
    "    result = spark.sql(f\"SELECT COUNT(*) as count FROM {DATABASE_NAME}.{TABLE_NAME}\")\n",
    "    total_rows = result.collect()[0][\"count\"]\n",
    "    print(f\"Total rows in table: {total_rows}\")\n",
    "\n",
    "    # Sample preview\n",
    "    print(\"\\nSample data:\")\n",
    "    sample_data = spark.sql(f\"SELECT * FROM {DATABASE_NAME}.{TABLE_NAME} LIMIT 5\")\n",
    "    sample_data.show()\n",
    "\n",
    "verify_table_data(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n",
    "print(\"Spark session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
